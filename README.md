# 592finalproject

# Weekly Retail Sales Forecasting (ISU CS 592 Final Project)

This repository contains code and data for a twoâ€stage hybrid forecasting pipeline that combines classical time-series decomposition (SARIMA) with modern gradient-boosted trees (LightGBM & XGBoost) to predict weekly Walmart store sales.  

---

## ğŸ“‚ Repository Structure

.
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ train.csv # Historical storeâ€“dept daily sales (2010â€“2012)
â”‚ â”œâ”€â”€ features.csv # Exogenous drivers: temperature, fuel price, CPI, unemployment, holiday flags
â”‚ â”œâ”€â”€ stores.csv # Store metadata: type, size
â”‚ â””â”€â”€ test.csv # Hold-out set for final submission
â”‚
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ neural_networks.ipynb # (Optional) Neural-network experiments on the same data
â”‚
â”œâ”€â”€ submissions/
â”‚ â”œâ”€â”€ submission_lightgbm.csv # LightGBM-only forecast
â”‚ â”œâ”€â”€ submission_xgboost.csv # XGBoost-only forecast
â”‚ â””â”€â”€ submission_ensemble.csv # Blended SARIMA+LightGBM+XGBoost forecast
â”‚
â”œâ”€â”€ README.md # You are here
â””â”€â”€ requirements.txt # Python dependencies

yaml


---

## ğŸš€ Quickstart

1. **Clone the repo**

   ```bash
   git clone https://github.com/tharani247/592finalproject.git
   cd 592finalproject
Create & activate a Python 3.10 environment


conda create -n retail_forecast python=3.10
conda activate retail_forecast
pip install -r requirements.txt
Run the end-to-end pipeline (optional: via Jupyter or Python scripts):

Data prep & feature engineering
See 01_data_prep.py & 04_features.py for merging, cleaning, encoding, and residual feature creation.

Exploratory analysis
Inspect 02_eda.ipynb or equivalent scripts for time-series plots, ACF/PACF, heatmaps.

SARIMA modeling
Fit & forecast with SARIMA in 03_sarima_model.py.

Residual modeling
Train LightGBM/XGBoost via 05_train_gbts.py (with time-series cross-validation).

Hyperparameter tuning
Launch Optuna search in 06_tune_optuna.py.

Ensembling & submission
Blend forecasts and export final CSV using 07_ensemble.py.

Inspect submissions
The .csv files in submissions/ follow Kaggleâ€™s â€œId, Weekly_Salesâ€ format.

ğŸ› ï¸ Requirements
All dependencies are pinned in requirements.txt. Key libraries include:

pandas >=2.2

numpy >=1.24

scikit-learn >=1.2

statsmodels >=0.14

lightgbm >=4.6

xgboost >=3.0

optuna >=3.1

matplotlib, seaborn

Install via:


pip install -r requirements.txt
ğŸ“ˆ Results
Hybrid SARIMA + LightGBM

Validation (10 % hold-out):

RMSE = 4 671.35

MAE = 2 815.58

MAPE = 12.4 %

Test (12 weeks hold-out): 43 % error reduction vs. SARIMA alone, 22 % vs. pure LightGBM.

Benchmarks:

SARIMA: RMSE = 8 234.00

Pure LightGBM: RMSE = 5 959.00

XGBoost: RMSE â‰ˆ 4 834.00

Ensemble: RMSE â‰ˆ 4 671.00

ğŸ“š References
Please see our paper (IEEE format) for full citations. Key works include:

Box et al., â€œTime Series Analysis: Forecasting and Control,â€ Wiley, 2015.

Zhang, â€œTime series forecasting using a hybrid ARIMA and neural network model,â€ Neurocomputing (2003).

Chen & Guestrin, â€œXGBoost: A scalable tree boosting system,â€ KDD 2016.

Ke et al., â€œLightGBM: A highly efficient gradient boosting decision tree,â€ NeurIPS 2017.

Akiba et al., â€œOptuna: A nextâ€generation hyperparameter optimization framework,â€ 2019.

ğŸ¤ Contact
Tharani Vadde tvadde@iastate.edu
Navyatha Gandlaparthi ngandlap@iastate.edu

Iowa State University, Ames, IA, USA
Course: CS 592 â€“ Advanced Machine Learning (Spring 2025)








